{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h9gpJPQOXAA4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This is the engine for defining the micro architecture of the backprop algo [As written by ../karpathy], along with some of my own\n",
        "custom additions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Value:\n",
        "    def __init__(self, data, child = () , op= '' ):\n",
        "        self.data = data\n",
        "        self.prev = set(child)\n",
        "        self.op = op\n",
        "        self._backward = lambda: None\n",
        "        self.gradient = 0\n",
        "    def __repr__(self):\n",
        "        return (f\"Value:(Data: {self.data})\")\n",
        "\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        new_val = Value(self.data + other.data, (other, self), '+')\n",
        "        def _backward():\n",
        "            self.gradient += new_val.gradient\n",
        "            other.gradient += new_val.gradient\n",
        "        new_val._backward = _backward\n",
        "        return new_val\n",
        "\n",
        "\n",
        "\n",
        "    def __neg__(self):\n",
        "      return self * -1\n",
        "\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return self + (-other)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        new_val = Value(self.data * other.data, (other, self), '*')\n",
        "        def _backward():\n",
        "            self.gradient += other.data*new_val.gradient\n",
        "            other.gradient += self.data*new_val.gradient\n",
        "        new_val._backward = _backward\n",
        "        return new_val\n",
        "\n",
        "    def tanh(self):\n",
        "        val = (math.exp(2*self.data)-1)/(math.exp(2*self.data)+1)\n",
        "        new_val = Value(val, (self, ), 'tanh')\n",
        "        def _backward():\n",
        "            self.gradient += (1-val**2)*new_val.gradient\n",
        "        new_val._backward = _backward\n",
        "        return new_val\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return self.data < other.data\n",
        "\n",
        "    def __le__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return self.data <= other.data\n",
        "\n",
        "    def exp(self):\n",
        "        val = np.exp(self.data)\n",
        "        new_val = Value(val, (self,), 'exp')\n",
        "        def _backward():\n",
        "            self.gradient +=new_val.data* new_val.gradient\n",
        "        new_val._backward = _backward\n",
        "        return new_val\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return self * other**-1\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "        def _backward():\n",
        "            self.gradient += other*(self.data**(other-1))*out.gradient\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self, alpha):\n",
        "        visited = set()\n",
        "        nodes = []\n",
        "        def build_top_graph(v):\n",
        "            if(v not in visited):\n",
        "                visited.add(v)\n",
        "                for i in v.prev:\n",
        "                    build_top_graph(i)\n",
        "                nodes.append(v)\n",
        "        build_top_graph(self)\n",
        "        self.gradient = 1\n",
        "        for i in reversed(nodes):\n",
        "            i._backward()\n",
        "\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return self*other\n",
        "\n",
        "    def log(self):\n",
        "        val = math.log(self.data)\n",
        "        new_val = Value(val, (self,), 'log')\n",
        "        def _backward():\n",
        "            self.gradient += (1 / self.data) * new_val.gradient\n",
        "        new_val._backward = _backward\n",
        "        return new_val\n",
        "\n",
        "    def relu(self):\n",
        "      new_val = Value(self.data if self.data> 0 else 0, (self, ), 'relu')\n",
        "      def _backward():\n",
        "         if(new_val==0):\n",
        "            self.gradient = 0\n",
        "         else:\n",
        "            self.gradient = 1*new_val.gradient\n",
        "\n",
        "      new_val._backward = _backward\n",
        "      return new_val\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class denseLayer:\n",
        "    #num_inputs is basically the length of each input batch.\n",
        "    def __init__(self, num_inputs, neurons, activation):\n",
        "        self.weights =  np.vectorize(Value)(np.random.uniform(-1, 1, (num_inputs,neurons)))\n",
        "        self.biases = np.vectorize(Value)(np.zeros((1,neurons)))\n",
        "        self.activation = activation\n",
        "    def forward(self, inputs):\n",
        "        if(len(inputs.shape)==1):\n",
        "            self.inputs = np.array([Value(i) for i in inputs])\n",
        "        else:\n",
        "            self.inputs = np.reshape([Value(j) for i in np.array(inputs) for j in i], (inputs.shape[0],inputs.shape[1]))\n",
        "        output = (np.dot(inputs, self.weights)+self.biases)\n",
        "        match self.activation:\n",
        "            case 'relu':\n",
        "                self.output = self.relu_forward(output)\n",
        "            case 'softmax':\n",
        "                self.output = self.softmax_forward(output)\n",
        "            case _:\n",
        "                raise \"Incorrect Activation Function\"\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def relu_forward(self, inputx):\n",
        "        if(len(inputx)==1):\n",
        "          return np.array([i.relu() for i in inputx[0]])\n",
        "        else:\n",
        "          return np.array([[i.relu() for i in j] for j in inputx], dtype = 'object')\n",
        "\n",
        "    def parameters_ret(self):\n",
        "        params = np.insert(self.weights, len(self.weights), self.biases, axis = 0)\n",
        "        return np.reshape(params, (1,(self.weights.shape[0]*self.weights.shape[1]+self.biases.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax_forward(self, inputs):\n",
        "        #Subbing the max input so as to avoid an overflow of values for exponentiation.\n",
        "        exp_vals = np.exp(inputs - np.max(inputs, axis = 1, keepdims= True))\n",
        "        summ = np.sum(exp_vals, axis=1, keepdims = True)\n",
        "        self.output = exp_vals / summ\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class final_loss:\n",
        "    def calculate(self, loss_array):\n",
        "        sumy = Value(0)\n",
        "        for i in loss_array:\n",
        "          sumy+=i\n",
        "        return sumy/Value(len(loss_array))\n",
        "\n",
        "\n",
        "class categorical_cross_entropy(final_loss):\n",
        "    def loss_function(self, ground_truth, predicted):\n",
        "        predicted = np.clip(predicted, Value(1e-7), Value(1-1e-7))\n",
        "        hello_loss = np.array([])\n",
        "        confidences = np.array([])\n",
        "        if (len(ground_truth.shape)==1):\n",
        "            hello_loss  = predicted[range(len(predicted)), ground_truth]\n",
        "            confidences = (-1)*np.log(hello_loss)\n",
        "        else:\n",
        "            hello_loss = [sum(-np.log(predicted[j])*ground_truth[j]) for j in range(len(predicted))]\n",
        "            confidences = hello_loss\n",
        "        self.loss = confidences\n",
        "        return self.loss"
      ],
      "metadata": {
        "id": "suL21mJWXDHx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the multi layer perceptron class, basically piecing together all the layers:\n",
        "\n",
        "class Multi_Layer_Perceptron:\n",
        "  def __init__(self, num_inputs, num_layers, num_neurons, neurons_in_last_layer, activation_, activation_final, input_data):\n",
        "\n",
        "       self.num_inputs = num_inputs\n",
        "       self.activation_ = activation_\n",
        "       self.activation_final = activation_final\n",
        "       self.input_data = input_data\n",
        "       self.layers = []\n",
        "       self.num_neurons = num_neurons\n",
        "       self.last_neurons = neurons_in_last_layer\n",
        "\n",
        "\n",
        "       for i in range(num_layers):\n",
        "            if(i==0):\n",
        "              self.layers.append(denseLayer(num_inputs, self.num_neurons, activation_))\n",
        "            elif(i==num_layers-1):\n",
        "              self.layers.append(denseLayer(self.num_neurons, self.last_neurons, activation_final))\n",
        "            else:\n",
        "              self.layers.append(denseLayer(self.num_neurons, self.num_neurons, activation_))\n",
        "\n",
        "\n",
        "  def forward(self):\n",
        "    prev_output = self.layers[0].forward(self.input_data)\n",
        "\n",
        "    for i in range(len(self.layers)):\n",
        "      if(i!=0):\n",
        "        prev_output = self.layers[i].forward(prev_output)\n",
        "    return self.layers[-1].output\n",
        "\n",
        "  def parameters_ret(self):\n",
        "    return [p for layer in self.layers for p in layer.parameters_ret()]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RIKKtjstdYKv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first partial derivative with respect to $ w^{L}$ which is the weight of the last layer:\n",
        "$$\\frac{\\mathrm{d}C_0}{\\mathrm{d}w_{jk}^{L}} = 2 \\sum_{j=0}^{n_L-1} (a_j^{L}-y_j). \\sigma^{'}(z_j^{L}). a_k^{L-1}.$$"
      ],
      "metadata": {
        "id": "K44-2oYzMz-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same thing is repeated for each param in all the layers that are present. The mathematical complexity of this doesnt appear in the code because we perform derivatives on the fundamental bit level mathematical operations [micrograd] which ends up saving a lot of time."
      ],
      "metadata": {
        "id": "Fk6SchbqNMwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "#This is some arbitrary data that we can essentially train our neural net on.\n",
        "\n",
        "\n",
        "X = pd.read_csv(r\"/content/backprop_data.csv\")\n",
        "ground_truth = X['is_canceled'][:500]\n",
        "\n",
        "\n",
        "input_state = X.to_numpy()\n",
        "input_data = input_state[:,1:]\n",
        "input_data = input_data[:500]\n",
        "\n"
      ],
      "metadata": {
        "id": "8GpIcPrPALVb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_data = np.array([[1,2,3,4,5], [2,3,4,5,6], [7,8,9,10,11]])\n",
        "ground_truth = np.array([1,0,2])\n"
      ],
      "metadata": {
        "id": "VqyAcfI8yAol"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of features per input example\n",
        "num_inputs = len(input_data[0])\n",
        "\n",
        "#each layer will have the same number of neurons [except the last one.]\n",
        "num_layers = 2\n",
        "num_neurons = 3\n",
        "\n",
        "#Because of the classification element\n",
        "neurons_in_last_layer = 3\n",
        "\n",
        "mlp = Multi_Layer_Perceptron(num_inputs, num_layers, num_neurons, neurons_in_last_layer,'relu', 'softmax', input_data)\n",
        "\n",
        "mlp_output = mlp.forward()"
      ],
      "metadata": {
        "id": "2zLuAUSuc9Z-"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = categorical_cross_entropy()\n",
        "final_cce_loss = loss.loss_function(ground_truth, mlp_output)"
      ],
      "metadata": {
        "id": "EayN9rGxXNE0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fl = final_loss()\n",
        "mean_loss = fl.calculate(final_cce_loss)\n",
        "mean_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q3nTrRVqDDK",
        "outputId": "9868facf-0276-4143-ea35-d5c7b2043d59"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value:(Data: 2.5316969343822198)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ip98zsQUKKUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class gradient_desc:\n",
        "    def __init__(self, learning_rate, ground_truth, mlp, orig_inputs, loss_func):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ground_truth = ground_truth\n",
        "        self.orig_inputs = orig_inputs\n",
        "        self.loss_obj = loss_func\n",
        "        self.mlp_obj = mlp\n",
        "\n",
        "\n",
        "    def forward_pass(self):\n",
        "        output1 = self.mlp_obj.forward()\n",
        "        return output1\n",
        "\n",
        "\n",
        "    def backward_pass(self, prev_loss, iter):\n",
        "       for i in self.mlp_obj.parameters_ret():\n",
        "            for j in i:\n",
        "              j.gradient = 0\n",
        "\n",
        "       prev_loss.backward(self.learning_rate)\n",
        "       for i in self.mlp_obj.parameters_ret():\n",
        "            for j in i:\n",
        "              j.data -= self.learning_rate*j.gradient\n",
        "\n",
        "       updated_results = self.forward_pass()\n",
        "       changed_loss = self.loss_obj.loss_function(self.ground_truth, updated_results)\n",
        "       fl = final_loss()\n",
        "       mean_loss = fl.calculate(changed_loss)\n",
        "       if(iter%10==0):\n",
        "          print(\"Loss in epoch \", iter, \" is: \", mean_loss)\n",
        "          print()\n",
        "       return mean_loss\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, epochs, loss_val):\n",
        "      for i in range(1, epochs):\n",
        "          loss_val = self.backward_pass(loss_val, i)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9XdznApdWDz"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate  = 0.01\n",
        "\n",
        "grad_obj = gradient_desc(learning_rate, ground_truth, mlp, input_data, loss)\n",
        "grad_obj.forward(1000, mean_loss)\n",
        "\n",
        "#After adding the zero grad, the loss minimizes fairly well.\n"
      ],
      "metadata": {
        "id": "lWgWADOLGk0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbe09ed-ecd3-4ce1-cf2c-9e2814a23edd"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss in epoch  10  is:  Value:(Data: 1.152699385580366)\n",
            "\n",
            "Loss in epoch  20  is:  Value:(Data: 1.004972784149535)\n",
            "\n",
            "Loss in epoch  30  is:  Value:(Data: 0.9803529088708213)\n",
            "\n",
            "Loss in epoch  40  is:  Value:(Data: 0.961662565935745)\n",
            "\n",
            "Loss in epoch  50  is:  Value:(Data: 0.9437794046686911)\n",
            "\n",
            "Loss in epoch  60  is:  Value:(Data: 0.926307025318503)\n",
            "\n",
            "Loss in epoch  70  is:  Value:(Data: 0.9090494742323731)\n",
            "\n",
            "Loss in epoch  80  is:  Value:(Data: 0.8918640564439482)\n",
            "\n",
            "Loss in epoch  90  is:  Value:(Data: 0.8746505309183488)\n",
            "\n",
            "Loss in epoch  100  is:  Value:(Data: 0.8573465494343965)\n",
            "\n",
            "Loss in epoch  110  is:  Value:(Data: 0.8497390034981476)\n",
            "\n",
            "Loss in epoch  120  is:  Value:(Data: 0.842949017843425)\n",
            "\n",
            "Loss in epoch  130  is:  Value:(Data: 0.8445532204539032)\n",
            "\n",
            "Loss in epoch  140  is:  Value:(Data: 0.8505174398044097)\n",
            "\n",
            "Loss in epoch  150  is:  Value:(Data: 0.8573643652775611)\n",
            "\n",
            "Loss in epoch  160  is:  Value:(Data: 0.8650376002544928)\n",
            "\n",
            "Loss in epoch  170  is:  Value:(Data: 0.8735000754066473)\n",
            "\n",
            "Loss in epoch  180  is:  Value:(Data: 0.8827085426681484)\n",
            "\n",
            "Loss in epoch  190  is:  Value:(Data: 0.8926047124877062)\n",
            "\n",
            "Loss in epoch  200  is:  Value:(Data: 0.895984093892173)\n",
            "\n",
            "Loss in epoch  210  is:  Value:(Data: 0.8906569884639302)\n",
            "\n",
            "Loss in epoch  220  is:  Value:(Data: 0.885338134157031)\n",
            "\n",
            "Loss in epoch  230  is:  Value:(Data: 0.8800143560547093)\n",
            "\n",
            "Loss in epoch  240  is:  Value:(Data: 0.874681763415516)\n",
            "\n",
            "Loss in epoch  250  is:  Value:(Data: 0.8693362928843176)\n",
            "\n",
            "Loss in epoch  260  is:  Value:(Data: 0.863973522929478)\n",
            "\n",
            "Loss in epoch  270  is:  Value:(Data: 0.8585886219789904)\n",
            "\n",
            "Loss in epoch  280  is:  Value:(Data: 0.8531762972398778)\n",
            "\n",
            "Loss in epoch  290  is:  Value:(Data: 0.8477307367289632)\n",
            "\n",
            "Loss in epoch  300  is:  Value:(Data: 0.8422455438862564)\n",
            "\n",
            "Loss in epoch  310  is:  Value:(Data: 0.8367136644201217)\n",
            "\n",
            "Loss in epoch  320  is:  Value:(Data: 0.8311273051437285)\n",
            "\n",
            "Loss in epoch  330  is:  Value:(Data: 0.8254778448887331)\n",
            "\n",
            "Loss in epoch  340  is:  Value:(Data: 0.8197557381800582)\n",
            "\n",
            "Loss in epoch  350  is:  Value:(Data: 0.8139504133192468)\n",
            "\n",
            "Loss in epoch  360  is:  Value:(Data: 0.8080501679992222)\n",
            "\n",
            "Loss in epoch  370  is:  Value:(Data: 0.8020420677583744)\n",
            "\n",
            "Loss in epoch  380  is:  Value:(Data: 0.7959118557250655)\n",
            "\n",
            "Loss in epoch  390  is:  Value:(Data: 0.789643886489802)\n",
            "\n",
            "Loss in epoch  400  is:  Value:(Data: 0.7832211028501825)\n",
            "\n",
            "Loss in epoch  410  is:  Value:(Data: 0.7766250817793281)\n",
            "\n",
            "Loss in epoch  420  is:  Value:(Data: 0.7698361851601525)\n",
            "\n",
            "Loss in epoch  430  is:  Value:(Data: 0.7628338608927407)\n",
            "\n",
            "Loss in epoch  440  is:  Value:(Data: 0.7555971491401068)\n",
            "\n",
            "Loss in epoch  450  is:  Value:(Data: 0.7481054533020002)\n",
            "\n",
            "Loss in epoch  460  is:  Value:(Data: 0.7403396302268438)\n",
            "\n",
            "Loss in epoch  470  is:  Value:(Data: 0.7322834315314216)\n",
            "\n",
            "Loss in epoch  480  is:  Value:(Data: 0.7239252793880443)\n",
            "\n",
            "Loss in epoch  490  is:  Value:(Data: 0.7152602805988071)\n",
            "\n",
            "Loss in epoch  500  is:  Value:(Data: 0.7062922767308308)\n",
            "\n",
            "Loss in epoch  510  is:  Value:(Data: 0.6970356172624992)\n",
            "\n",
            "Loss in epoch  520  is:  Value:(Data: 0.6875162691218872)\n",
            "\n",
            "Loss in epoch  530  is:  Value:(Data: 0.6777718926012988)\n",
            "\n",
            "Loss in epoch  540  is:  Value:(Data: 0.6678506576775947)\n",
            "\n",
            "Loss in epoch  550  is:  Value:(Data: 0.6578088334003345)\n",
            "\n",
            "Loss in epoch  560  is:  Value:(Data: 0.6477074770333516)\n",
            "\n",
            "Loss in epoch  570  is:  Value:(Data: 0.6376087616018096)\n",
            "\n",
            "Loss in epoch  580  is:  Value:(Data: 0.627572521131434)\n",
            "\n",
            "Loss in epoch  590  is:  Value:(Data: 0.6176534582770857)\n",
            "\n",
            "Loss in epoch  600  is:  Value:(Data: 0.6078992292463652)\n",
            "\n",
            "Loss in epoch  610  is:  Value:(Data: 0.5983494015792079)\n",
            "\n",
            "Loss in epoch  620  is:  Value:(Data: 0.5890351399663463)\n",
            "\n",
            "Loss in epoch  630  is:  Value:(Data: 0.5799794242866978)\n",
            "\n",
            "Loss in epoch  640  is:  Value:(Data: 0.5711976149771806)\n",
            "\n",
            "Loss in epoch  650  is:  Value:(Data: 0.5663837375173691)\n",
            "\n",
            "Loss in epoch  660  is:  Value:(Data: 0.5635488827181014)\n",
            "\n",
            "Loss in epoch  670  is:  Value:(Data: 0.561158386915107)\n",
            "\n",
            "Loss in epoch  680  is:  Value:(Data: 0.5590981402953699)\n",
            "\n",
            "Loss in epoch  690  is:  Value:(Data: 0.5573098375652523)\n",
            "\n",
            "Loss in epoch  700  is:  Value:(Data: 0.5557571167480224)\n",
            "\n",
            "Loss in epoch  710  is:  Value:(Data: 0.5544135487140742)\n",
            "\n",
            "Loss in epoch  720  is:  Value:(Data: 0.5532580201039097)\n",
            "\n",
            "Loss in epoch  730  is:  Value:(Data: 0.5522727119785382)\n",
            "\n",
            "Loss in epoch  740  is:  Value:(Data: 0.5514420614078471)\n",
            "\n",
            "Loss in epoch  750  is:  Value:(Data: 0.5507521455318704)\n",
            "\n",
            "Loss in epoch  760  is:  Value:(Data: 0.550190275929213)\n",
            "\n",
            "Loss in epoch  770  is:  Value:(Data: 0.5497447103231015)\n",
            "\n",
            "Loss in epoch  780  is:  Value:(Data: 0.5494044333457619)\n",
            "\n",
            "Loss in epoch  790  is:  Value:(Data: 0.5491589775942092)\n",
            "\n",
            "Loss in epoch  800  is:  Value:(Data: 0.5489982662818584)\n",
            "\n",
            "Loss in epoch  810  is:  Value:(Data: 0.5489124646618677)\n",
            "\n",
            "Loss in epoch  820  is:  Value:(Data: 0.5488918310276446)\n",
            "\n",
            "Loss in epoch  830  is:  Value:(Data: 0.5489265603674465)\n",
            "\n",
            "Loss in epoch  840  is:  Value:(Data: 0.5490066151565222)\n",
            "\n",
            "Loss in epoch  850  is:  Value:(Data: 0.5491215386404908)\n",
            "\n",
            "Loss in epoch  860  is:  Value:(Data: 0.5481794087617707)\n",
            "\n",
            "Loss in epoch  870  is:  Value:(Data: 0.5474118598562762)\n",
            "\n",
            "Loss in epoch  880  is:  Value:(Data: 0.5468403917051905)\n",
            "\n",
            "Loss in epoch  890  is:  Value:(Data: 0.5463712368687443)\n",
            "\n",
            "Loss in epoch  900  is:  Value:(Data: 0.5459546045681286)\n",
            "\n",
            "Loss in epoch  910  is:  Value:(Data: 0.5455638853970868)\n",
            "\n",
            "Loss in epoch  920  is:  Value:(Data: 0.5451851534116736)\n",
            "\n",
            "Loss in epoch  930  is:  Value:(Data: 0.5448114236742923)\n",
            "\n",
            "Loss in epoch  940  is:  Value:(Data: 0.5444394265279374)\n",
            "\n",
            "Loss in epoch  950  is:  Value:(Data: 0.5440678106317189)\n",
            "\n",
            "Loss in epoch  960  is:  Value:(Data: 0.5436961688687212)\n",
            "\n",
            "Loss in epoch  970  is:  Value:(Data: 0.5433245294387496)\n",
            "\n",
            "Loss in epoch  980  is:  Value:(Data: 0.5429531010797464)\n",
            "\n",
            "Loss in epoch  990  is:  Value:(Data: 0.5425821515996582)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHZtgMwN9ubO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}