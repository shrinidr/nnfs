{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward Neural Networks w/backprop and gradient descent from scratch using only Numpy and other scientific libraries in python excluding any pre-built machine learning \n",
    "libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using objects to define layers\n",
    "X = np.array([[1,2,3,2.5], \n",
    "             [2,5,-1,2],\n",
    "             [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "class denseLayer:\n",
    "    #num_inputs is basically the length of each input batch.\n",
    "    def __init__(self, num_inputs, neurons):\n",
    "        self.weights = np.random.rand(num_inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights)+self.biases\n",
    "        \n",
    "class relu:\n",
    "    def forward(self, inputx):\n",
    "        self.output =  np.maximum(inputx, 0)\n",
    "        \n",
    "class soft_max:\n",
    "    def forward(self, inputs):\n",
    "        #Subbing the max input so as to avoid an overflow of values for exponentiation.\n",
    "        exp_vals = np.exp(inputs - np.max(inputs, axis = 1, keepdims= True))\n",
    "        summ = np.sum(exp_vals, axis=1, keepdims = True)\n",
    "        self.output = exp_vals / summ\n",
    "\n",
    "class categorical_cross_entropy(final_loss):\n",
    "    def loss_function(self, ground_truth, predicted):\n",
    "        predicted = np.clip(predicted, 1e-7, 1-1e-7)\n",
    "        hello_loss = np.array([])\n",
    "        confidences = np.array([])\n",
    "        if (len(ground_truth.shape)==1):\n",
    "            hello_loss  = predicted[range(len(predicted)), ground_truth]\n",
    "            confidences = -np.log(hello_loss)\n",
    "        else:\n",
    "            hello_loss = [sum(-np.log(predicted[j])*ground_truth[j]) for j in range(len(predicted))]\n",
    "            confidences = hello_loss\n",
    "        self.loss = confidences\n",
    "\n",
    "class final_loss:\n",
    "    def calculate(self, loss_array):\n",
    "        return np.mean(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some simple data\n",
    "from nnfs.datasets import spiral_data\n",
    "X, y = spiral_data(100,3)\n",
    "\n",
    "#X consists of 300 training samples, with each sample having 2 features.\n",
    "#y basically consists of 3 classes. So, the shape is 300,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1 = denseLayer(2, 5)\n",
    "activation_1 = relu()\n",
    "layer_1.forward(X)\n",
    "output_layer_1 = layer_1.output\n",
    "\n",
    "#This is the output after passing it through the reLU activation\n",
    "\n",
    "activation_1.forward(output_layer_1)\n",
    "final_output_l1 = activation_1.output\n",
    "final_output_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 1 with 3 neurons and relu activation\n",
    "\n",
    "dense1 = denseLayer(2, 3)\n",
    "act1 = relu()\n",
    "dense1.forward(X)\n",
    "act1.forward(dense1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 2 with 3 neurons and softmax actication.\n",
    "\n",
    "dense2 = denseLayer(3, 3)\n",
    "act2 = soft_max()\n",
    "dense2.forward(act1.output)\n",
    "act2.forward(dense2.output)\n",
    "final_second_output =act2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output from the softmax\n",
    "batch_1_sm = final_second_output[:5]\n",
    "\n",
    "#Ground truth (say):\n",
    " \n",
    "ground_truths = np.array([0,1,2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ground truths can be one hot encoded, as well as a sparse matrix\n",
    "\n",
    "CCE = categorical_cross_entropy()\n",
    "CCE.loss_function(ground_truths, batch_1_sm)\n",
    "loss_array = CCE.loss\n",
    "#We can use the final_loss to calculate the actual mean loss across all the input batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1008854252083624"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CCE.calculate(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
